{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPyTurm/1mTVEgVx/2j/bBX"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["------This Jupyter notebook is authored by J. Antonio Sidaoui, jas2545@columbia.edu------"],"metadata":{"id":"af51NVN5-QRC"}},{"cell_type":"markdown","source":["# Diffusion Maps Implementation"],"metadata":{"id":"FgjvLFHzHtg_"}},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn import preprocessing\n","from numpy import genfromtxt\n","from numpy import linalg"],"metadata":{"id":"A7JWOVWtHd_-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Function to ensure covariance matrices remain positive definite for numerical stability\n","\n","def ensure_positive_definite(matrix, min_eigenvalue=1e-6):\n","    \"\"\"\n","    Ensures the matrix is positive definite by adjusting its eigenvalues.\n","\n","    Parameters:\n","    matrix (numpy.ndarray): The input matrix to be checked and adjusted.\n","    min_eigenvalue (float): The minimum eigenvalue threshold.\n","\n","    Returns:\n","    numpy.ndarray: A positive definite matrix.\n","    \"\"\"\n","    # Eigenvalue decomposition\n","    eigenvalues, eigenvectors = np.linalg.eigh(matrix)\n","    # Clip eigenvalues to ensure they are not too small\n","    eigenvalues = np.clip(eigenvalues, min_eigenvalue, None)\n","    # Reconstruct the matrix\n","    matrix_pd = (eigenvectors @ np.diag(eigenvalues)) @ eigenvectors.T\n","    return matrix_pd"],"metadata":{"id":"nZS4SW5nHd9x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def compute_distances(X):\n","    '''\n","    Constructs a distance matrix from data set, assumes Euclidean distance\n","\n","    Inputs:\n","        X       a numpy array of size n x p holding the data set (n observations, p features)\n","\n","    Outputs:\n","        D       a numpy array of size n x n containing the euclidean distances between points\n","\n","    '''\n","\n","    # Initialize distance matrix D\n","    n = np.shape(X)[0]\n","    D = np.empty([n, n])\n","\n","    # Precompute the inverses for efficiency\n","    inverses = [np.linalg.pinv(np.matmul(np.transpose(X[0:i, :]), X[0:i, :])) for i in range(n)]\n","\n","    for i in range(n):\n","      for j in range(n):\n","        diff = X[i, :] - X[j, :]\n","        D[i, j] = 0.5 * np.linalg.multi_dot([diff, (inverses[i] + inverses[j]), diff.T]) # Compute modified Mahalanbis distance between data points i and j\n","                                                                                         # Note that data points are in R^p and are rows of the matrix\n","    # return distance matrix\n","    return D"],"metadata":{"id":"fczsE6eIHeCD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def compute_affinity_matrix(D, kernel_type, sigma=None, k=None):\n","    '''\n","    Construct an affinity matrix from a distance matrix via gaussian kernel.\n","\n","    Inputs:\n","        D               a numpy array of size n x n containing the distances between points\n","        kernel_type     a string, either \"gaussian\" or \"adaptive\".\n","                            If kernel_type = \"gaussian\", then sigma must be a positive number\n","                            If kernel_type = \"adaptive\", then k must be a positive integer\n","        sigma           the non-adaptive gaussian kernel parameter\n","        k               the adaptive kernel parameter\n","\n","    Outputs:\n","        W       a numpy array of size n x n that is the affinity matrix\n","\n","    '''\n","    if kernel_type=='gaussian':\n","        if sigma<=0:  # Check if sigma is valid\n","            return 'error, sigma must be a positive number' # return error if not\n","        else:\n","            W=np.exp((-(D**2))/(sigma**2)) # Compute affinity matrix using Gaussian Kernel\n","\n","    if kernel_type=='adaptive':\n","        if type(k)!=int or k<=0: # Check if t is valid\n","            return 'error, k must be a positive integer' # return error if not\n","        else:\n","            n=np.shape(D)[0]\n","            W=np.empty([n,n]) # Initialize W\n","            D_sort=np.sort(D, axis=1) # Sort rows of D from smallest to largest\n","            for i in range(n):\n","                for j in range(n):\n","                    # the denominators here take the k'th element in the i'th or j'th row of the sorted D\n","                    W[i, j]=.5*(np.exp(-(D[i, j]**2)/(D_sort[i, :][k]**2))+\n","                                np.exp(-(D[i, j]**2)/(D_sort[j, :][k]**2)))\n","\n","    # return the affinity matrix\n","    return W"],"metadata":{"id":"8iKxir_CH0LO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def diff_map_info(W):\n","    '''\n","    Construct the information necessary to easily construct diffusion map for any t\n","\n","    Inputs:\n","        W           a numpy array of size n x n containing the affinities between points\n","\n","    Outputs:\n","\n","        diff_vec    a numpy array of size n x n-1 containing the n-1 nontrivial eigenvectors of Markov matrix as columns\n","        diff_eig    a numpy array of size n-1 containing the n-1 nontrivial eigenvalues of Markov matrix\n","\n","        We assume the convention that the coordinates in the diffusion vectors are in descending order\n","        according to eigenvalues.\n","    '''\n","    # Compute the matrix of row sums D^(-1/2)\n","    D=np.diag((np.sum(W, axis=1))**(-.5))\n","\n","    # Construct symmetric matrix M_s\n","    M_s=D@W@D\n","\n","    # Find eigenvalues and eigenvectors of M_s\n","    w, v=np.linalg.eigh(M_s)\n","\n","    # Sort eigenvalues and corresponding eigenvectors in descending order\n","    eigvals=-np.sort(-w)\n","    eigvecs=v[:, eigvals.argsort()]\n","\n","    # Chop off the trivial eigenvector and eigenvalue\n","    eigvals=eigvals[1:]\n","    eigvecs=eigvecs[:, 1:]\n","\n","    # Get the eigenpairs\n","    diff_eig=eigvals # eigenvalues of Markov matrix are the same as those of M_s\n","    n=np.shape(W)[1]\n","    diff_vec=np.empty([n, n-1]) # initialize diff_vec\n","\n","    # Compute normalized eigenvectors of the Markov matrix\n","    for i in range(np.shape(eigvecs)[1]):\n","        diff_vec[:, i]=(D@eigvecs[:, i])/np.linalg.norm(D@eigvecs[:, i])\n","\n","    # return the info for diffusion maps\n","    return diff_vec, diff_eig"],"metadata":{"id":"dD2fcIqnH0Nb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_diff_map(diff_vec, diff_eig, t):\n","    '''\n","    Construct a diffusion map at t from eigenvalues and eigenvectors of Markov matrix\n","\n","    Inputs:\n","        diff_vec    a numpy array of size n x n-1 containing the n-1 nontrivial eigenvectors of Markov matrix as columns\n","        diff_eig    a numpy array of size n-1 containing the n-1 nontrivial eigenvalues of Markov matrix\n","        t           diffusion time parameter t\n","\n","    Outputs:\n","        diff_map    a numpy array of size n x n-1, the diffusion map defined for t\n","    '''\n","    # Raise each eigenvalue to the t'th power and multiply the corresponding eigenvector by it\n","    diff_map=diff_vec*(diff_eig**t)\n","\n","    return diff_map"],"metadata":{"id":"6ByeuyPmH0Pa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Choose k dominant eigenvalues as dimension of embedding space and number of latent variables\n","\n","def embedding_dimension(diff_eig):\n","  consecutive_distances = np.abs(np.diff(diff_eig))\n","  dimension=np.argmax(consecutive_distances) # find the largest spectral gap to set the dimension\n","  return dimension"],"metadata":{"id":"87U5a5p4H0RP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Conditional Sampling & JDKF"],"metadata":{"id":"CNKmUP3oIUxc"}},{"cell_type":"code","source":["def is_positive_semidefinite(matrix):\n","    \"\"\"Checks if a matrix is positive semidefinite.\"\"\"\n","    try:\n","        # Attempt Cholesky decomposition\n","        np.linalg.cholesky(matrix)\n","        return True  # If successful, it's positive semidefinite\n","    except np.linalg.LinAlgError:\n","        return False  # If Cholesky fails, it's not"],"metadata":{"id":"QtC3FnWgIUPb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Sample diffusion coordinates at t+1 conditional on the scenario and information up to time t\n","\n","def conditional_sample_H(F, Q, H, psi_t, fixed_indices, fixed_values):\n","    \"\"\"\n","    Sample psi_{t+1} given psi_t and a subset of coordinates of H*psi_{t+1} being fixed.\n","\n","    Parameters:\n","    F (numpy.ndarray): State transition matrix.\n","    Q (numpy.ndarray): Process covariance matrix.\n","    H (numpy.ndarray): Lifting operator from embedding space to original space.\n","    psi_t (numpy.ndarray): Current state.\n","    fixed_indices (list of int): Indices of the coordinates of H*psi_{t+1} that are fixed (factors to be stressed).\n","    fixed_values (numpy.ndarray): Values to which the coordinates of H*psi_{t+1} are fixed (scenario vector).\n","\n","    Returns:\n","    numpy.ndarray: Sampled psi_{t+1} conditioned on the fixed values.\n","    \"\"\"\n","    # Predict the mean and covariance of psi_{t+1} given psi_t\n","    mu_t1 = F @ psi_t\n","    cov_t1 = Q\n","\n","    # Predict the mean and covariance in the measurement space\n","    mu_t1_obs = H @ mu_t1\n","    cov_t1_obs = H @ cov_t1 @ H.T\n","\n","    # Partition the mean and covariance based on fixed_indices in the measurement space\n","    all_indices_obs = np.arange(len(mu_t1_obs))\n","    free_indices_obs = np.setdiff1d(all_indices_obs, fixed_indices)\n","\n","    mu_t1_obs_fixed = mu_t1_obs[fixed_indices]\n","    mu_t1_obs_free = mu_t1_obs[free_indices_obs]\n","\n","    cov_t1_obs_ff = cov_t1_obs[np.ix_(free_indices_obs, free_indices_obs)]\n","    cov_t1_obs_fi = cov_t1_obs[np.ix_(free_indices_obs, fixed_indices)]\n","    cov_t1_obs_if = cov_t1_obs[np.ix_(fixed_indices, free_indices_obs)]\n","    cov_t1_obs_ii = cov_t1_obs[np.ix_(fixed_indices, fixed_indices)]\n","\n","    if not is_positive_semidefinite(cov_t1_obs_ii):\n","        cov_t1_obs_ii = ensure_positive_definite(cov_t1_obs_ii)\n","\n","    # Conditional mean and covariance in the measurement space\n","    mu_cond_obs = mu_t1_obs_free + cov_t1_obs_fi @ np.linalg.inv(cov_t1_obs_ii) @ (fixed_values - mu_t1_obs_fixed)\n","    cov_cond_obs = cov_t1_obs_ff - cov_t1_obs_fi @ np.linalg.inv(cov_t1_obs_ii) @ cov_t1_obs_if\n","\n","    # Convert the conditional mean and covariance back to the latent space\n","    H_free = H[free_indices_obs]\n","    mu_cond_latent = np.linalg.pinv(H_free) @ mu_cond_obs\n","    cov_cond_latent = np.linalg.pinv(H_free) @ cov_cond_obs @ np.linalg.pinv(H_free).T\n","\n","    if not is_positive_semidefinite(cov_cond_latent):\n","        cov_cond_latent = ensure_positive_definite(cov_cond_latent)\n","\n","\n","    # Sample from the conditional distribution\n","    psi_t1_free_sampled = np.random.multivariate_normal(mu_cond_latent, cov_cond_latent)\n","\n","    # Construct the full psi_{t+1} sample\n","    psi_t1 = np.empty_like(mu_t1)\n","    psi_t1 = psi_t1_free_sampled\n","\n","    return psi_t1"],"metadata":{"id":"tBk5b8n_IURw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def kalman_filter_joint(y, z, F, H_y, Q, R_y, A, R_z, R_yz, x0, P0):\n","  \"\"\"\n","    Kalman filter with joint update using both y_t and z_t.\n","\n","    Parameters:\n","    - y: Observations of y_t (n_timesteps, n_y)\n","    - z: Observations of z_t (n_timesteps, n_z)\n","    - F: State transition matrix (n_x, n_x)\n","    - H_y: Lifting operator matrix for y_t (n_y, n_x)\n","    - Q: Process noise covariance (n_x, n_x)\n","    - R_y: Observation noise covariance for y_t (n_y, n_y)\n","    - A: Matrix linking y_t to z_t (n_z, n_y)\n","    - R_z: Observation noise covariance for z_t (n_z, n_z)\n","    - R_yz: Covariance between y_t and z_t (n_y, n_z)\n","    - x0: Initial state estimate (n_x,)\n","    - P0: Initial state covariance (n_x, n_x)\n","\n","    Returns:\n","    - x_est: Estimated states over time (n_timesteps, n_x)\n","    - P_est: Estimated state covariance over time (n_timesteps, n_x, n_x)\n","    - log_likelihood: Total log-likelihood for the observations\n","    \"\"\"\n","    T = y.shape[0]\n","    n_x = F.shape[0]\n","\n","    x_est = np.zeros((T, n_x))\n","    P_est = np.zeros((T, n_x, n_x))\n","\n","    x_t = x0\n","    P_t = P0\n","    log_likelihood = 0.0\n","\n","    H_joint = np.vstack([H_y, A @ H_y])\n","    R_joint = np.block([\n","        [R_y,    R_yz],\n","        [R_yz.T, R_z ]\n","    ])\n","    R_joint = ensure_positive_definite(R_joint)\n","\n","    for t in range(T):\n","        z_joint_t = np.hstack([y[t], z[t]])\n","\n","        x_pred = F @ x_t\n","        P_pred = F @ P_t @ F.T + Q\n","\n","        innovation = z_joint_t - H_joint @ x_pred\n","        S = H_joint @ P_pred @ H_joint.T + R_joint\n","        S = ensure_positive_definite(S)\n","\n","        K = P_pred @ H_joint.T @ np.linalg.pinv(S)\n","        x_t = x_pred + K @ innovation\n","        P_t = (np.eye(n_x) - K @ H_joint) @ P_pred\n","        P_t = ensure_positive_definite(P_t)\n","\n","        x_est[t] = x_t\n","        P_est[t] = P_t\n","\n","        ll = -0.5 * (np.log(np.linalg.det(S) + 1e-10) +\n","                     innovation.T @ np.linalg.pinv(S) @ innovation +\n","                     len(z_joint_t) * np.log(2 * np.pi))\n","        log_likelihood += ll\n","\n","    return x_est, P_est, log_likelihood"],"metadata":{"id":"9vIH8baJI3bh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def rts_smoother(x_filt, P_filt, F, Q):\n","  \"\"\"\n","    Implementation of the RTS smoother.\n","\n","    Parameters:\n","    - x_filt: Filtered states (n_timesteps, n_x)\n","    - P_filt: Filtered state covariances (n_timesteps, n_x, n_x)\n","    - F: State transition matrix (n_x, n_x)\n","    - Q: Process noise covariance (n_x, n_x)\n","    Returns:\n","    - x_smooth: Smoothed states (n_timesteps, n_x)\n","    - P_smooth: Smoothed state covariances (n_timesteps, n_x, n_x)\n","    \"\"\"\n","    T, n = x_filt.shape\n","    x_smooth = np.zeros_like(x_filt)\n","    P_smooth = np.zeros_like(P_filt)\n","\n","    x_smooth[-1] = x_filt[-1]\n","    P_smooth[-1] = P_filt[-1]\n","\n","    for t in reversed(range(T - 1)):\n","        P_pred = F @ P_filt[t] @ F.T + Q\n","        G = P_filt[t] @ F.T @ np.linalg.pinv(P_pred)\n","\n","        x_smooth[t] = x_filt[t] + G @ (x_smooth[t+1] - F @ x_filt[t])\n","        P_smooth[t] = P_filt[t] + G @ (P_smooth[t+1] - P_pred) @ G.T\n","\n","    return x_smooth, P_smooth"],"metadata":{"id":"SrfGoBt1I3eQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def maximization_step(y, z, x_smooth, H_y):\n","    \"\"\"\n","    Perform the maximization step of the JDKF algorithm.\n","\n","    Parameters:\n","    - y: Observations of y_t (n_timesteps, n_y)\n","    - z: Observations of z_t (n_timesteps, n_z)\n","    - x_smooth: Smoothed states (n_timesteps, n_x)\n","    - H_y: Lifting operator matrix for y_t (n_y, n_x)\n","    Returns:\n","    - A: Matrix linking y_t to z_t (n_z, n_y)\n","    - R_z: Observation noise covariance for z_t (n_z, n_z)\n","    - R_y: Observation noise covariance for y_t (n_y, n_y)\n","    - R_yz: Covariance between y_t and z_t (n_y, n_z)\n","    \"\"\"\n","    T = x_smooth.shape[0]\n","    Hx = (H_y @ x_smooth.T).T  # (T, n_y)\n","\n","    # Estimate A: z = A (H_y x)\n","    A = np.linalg.lstsq(Hx, z, rcond=None)[0].T  # (n_z, n_y)\n","\n","    # Residuals\n","    res_y = y - Hx\n","    res_z = z - (A @ Hx.T).T\n","\n","    # Covariance estimates\n","    R_y  = (res_y.T @ res_y) / T\n","    R_z  = (res_z.T @ res_z) / T\n","    R_yz = (res_y.T @ res_z) / T\n","\n","    return A, R_z, R_y, R_yz"],"metadata":{"id":"7sOwijOsI3gF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def em_kalman_filter(y, z, F, H_y, Q, x0, P0, A_init=None, R_y_init=None, R_z_init=None, R_yz_init=None, max_iters=100, tol=1e-6):\n","  \"\"\"\n","    Implementation of the Expectation-Maximization (EM) algorithm for the JDKF.\n","\n","    Parameters:\n","    - y: Observations of y_t (n_timesteps, n_y)\n","    - z: Observations of z_t (n_timesteps, n_z)\n","    - F: State transition matrix (n_x, n_x)\n","    - H_y: Lifting operator matrix for y_t (n_y, n_x)\n","    - Q: Process noise covariance (n_x, n_x)\n","    - x0: Initial state estimate (n_x,)\n","    - P0: Initial state covariance (n_x, n_x)\n","    - A_init: Initial matrix linking y_t to z_t (n_z, n_y)\n","    - R_y_init: Initial observation noise covariance for y_t (n_y, n_y)\n","    - R_z_init: Initial observation noise covariance for z_t (n_z, n_z)\n","    - R_yz_init: Initial covariance between y_t and z_t (n_\n","    Returns:\n","    - A: Matrix linking y_t to z_t (n_z, n_\n","    - R_z: Observation noise covariance for z_t (n_z, n_z)\n","    - R_y: Observation noise covariance for y_t (n_y, n_y)\n","    - R_yz: Covariance between y_t and z_t (n_y, n_z)\n","    - x_smooth: Smoothed states (n_timesteps, n_x)\n","    - x_filt: Filtered states (n_timesteps, n_x)\n","    - log_likelihoods: Log-likelihoods at each iteration\n","    \"\"\"\n","    T, n_y = y.shape\n","    n_z = z.shape[1]\n","\n","    A = np.random.randn(n_z, n_y) if A_init is None else A_init\n","    R_y = np.eye(n_y) * 0.1 if R_y_init is None else R_y_init\n","    R_z = np.eye(n_z) * 0.1 if R_z_init is None else R_z_init\n","    R_yz = np.zeros((n_y, n_z)) if R_yz_init is None else R_yz_init\n","\n","    log_likelihoods = []\n","\n","    for it in range(max_iters):\n","        x_filt, P_filt, log_lik = kalman_filter_joint(y, z, F, H_y, Q, R_y, A, R_z, R_yz, x0, P0)\n","        x_smooth, P_smooth = rts_smoother(x_filt, P_filt, F, Q)\n","\n","        log_likelihoods.append(log_lik)\n","        print(f\"Iter {it}, log-likelihood: {log_lik:.4f}\")\n","\n","        # Use relative change for convergence\n","        if it > 0:\n","            delta_ll = abs(log_likelihoods[-1] - log_likelihoods[-2])\n","            rel_change = delta_ll / max(abs(log_likelihoods[-2]), 1e-10)\n","            print(f\"Relative log-likelihood change: {rel_change}\")\n","            if rel_change < tol:  # threshold\n","                print(f\"Converged by relative log-likelihood change at iteration {it}\")\n","                break\n","\n","        A, R_z, R_y, R_yz = maximization_step(y, z, x_smooth, H_y)\n","\n","    return A, R_z, R_y, R_yz, x_smooth, x_filt, log_likelihoods"],"metadata":{"id":"wRLlixQ4HeEC"},"execution_count":null,"outputs":[]}]}